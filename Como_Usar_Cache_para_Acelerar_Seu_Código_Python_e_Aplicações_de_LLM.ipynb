{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "3xnFNpv16WaG"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Como Usar Cache para Acelerar Seu Código Python e Aplicações de LLM**\n",
        "---\n",
        "\n",
        "**Notebook baseado no arquivo:** \"How to Use Caching to Speed Up Your Python Code & LLM Application\";\n",
        "\n",
        "**Autor:** *Youssef Hosni;*\n",
        "\n",
        "**Data de Publicação:** 14 de julho de 2024.\n",
        "\n",
        "---\n",
        "O artigo discute a importância do cache na melhoria da performance de aplicações, especialmente em contextos onde a latência pode afetar significativamente a experiência do usuário. Serão apresentados conceitos de cache, exemplos práticos de implementação em Python e a aplicação em modelos de Linguagem Natural (LLM).\n"
      ],
      "metadata": {
        "id": "JZqr0e0yv8aj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **EQUIPE**\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "b-IqtKrmmkAn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Integrante 01:**\n",
        "\n",
        "André Ribeiro Thasmo - 11202130043\n",
        "\n",
        "**Integrante 02:**\n",
        "\n",
        "Bheatriz Almeida Santos de Jesus - 11202131693\n",
        "\n",
        "**Integrante 03:**\n",
        "\n",
        "Caio Alexandre Sampaio Diniz - 11202130822"
      ],
      "metadata": {
        "id": "L-Adn7xCmaQh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Sumário**\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "4XzXY1PSwE2x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. O que é cache na programação?\n",
        "2. Benefícios do Cache\n",
        "3. Usos comuns para o cache\n",
        "4. Estratégias comuns de cache\n",
        "5. Comparação de uso de Caching usando Fibonacci\n",
        "6. Utilização de Caching para Aumentar a Velocidade de uma Aplicação de LLM\n",
        "7. Comparação de Caching em Consultas HTTP\n",
        "8. Conclusões"
      ],
      "metadata": {
        "id": "WyVXiiEBwI-e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "##1. O que é cache na programação?\n",
        "O cache é um mecanismo utilizado para melhorar o desempenho de qualquer aplicação. Em termos técnicos, cache é o armazenamento temporário de dados, que podem ser recuperados posteriormente de forma rápida. Um cache é um espaço de armazenamento rápido (geralmente temporário) onde dados acessados frequentemente são mantidos para acelerar o desempenho do sistema e diminuir os tempos de acesso.\n",
        "\n",
        "Por exemplo, o cache de um computador é um chip de memória pequeno, mas rápido (geralmente uma SRAM), localizado entre a CPU e o chip de memória principal (geralmente uma DRAM). Quando a CPU precisa acessar dados, ela primeiro verifica o cache. Se os dados estiverem no cache, ocorre um cache hit, e os dados são lidos do cache em vez de serem lidos da memória principal, que é relativamente mais lenta. Isso resulta em tempos de acesso reduzidos e desempenho aprimorado.\n",
        "\n",
        "##2. Benefícios do Cache\n",
        "- **Tempo de Acesso Reduzido:** Acelera o acesso a dados frequentemente usados.\n",
        "- **Carga do Sistema Reduzida:** Menos solicitações a fontes externas.\n",
        "- **Melhora da Experiência do Usuário:** Interações mais rápidas e fluidas.\n",
        "\n",
        "##3. Usos Comuns do Cache\n",
        "- Conteúdo Web: Páginas e imagens acessadas frequentemente.\n",
        "- Consultas a Banco de Dados: Resultados de consultas comuns.\n",
        "- Respostas de APIs: Evita solicitações de rede repetidas.\n",
        "- Dados de Sessão: Informações específicas do usuário.\n",
        "- Modelos de Machine Learning: Resultados intermediários e conjuntos de dados.\n",
        "- Configurações: Dados de configuração da aplicação.\n"
      ],
      "metadata": {
        "id": "megLB61AnPbX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##4. Estratégias Comuns de Cache\n",
        "\n",
        "Diferentes estratégias de cache podem ser elaboradas com base em padrões específicos de acesso a dados espaciais ou temporais, que podem ser vistos a seguir:\n",
        "\n",
        "\n",
        "\n",
        "#### **4.1. Cache-Aside (Carregamento Preguiçoso):** Os dados são carregados no cache apenas quando solicitados. Se os dados não forem encontrados no cache (um cache miss), eles são buscados na fonte, armazenados no cache e depois retornados ao solicitante.\n",
        "\n",
        "#### **4.2. Write-Through:** Toda vez que os dados são gravados no banco de dados, eles são simultaneamente gravados no cache. Isso garante que o cache sempre tenha os dados mais atualizados, mas pode introduzir latência adicional na gravação.\n",
        "\n",
        "#### **4.3. Write-Back (Write-Behind):** Os dados são gravados no cache e imediatamente confirmados ao solicitante, com o cache escrevendo os dados de forma assíncrona no banco de dados. Isso melhora o desempenho de gravação, mas há risco de perda de dados se o cache falhar antes que a gravação no banco de dados seja concluída.\n",
        "\n",
        "#### **4.4. Read-Through:** A aplicação interage apenas com o cache, e o cache é responsável por carregar os dados da fonte, caso ainda não estejam armazenados.\n",
        "\n",
        "#### **4.5. Time-to-Live (TTL):** Os dados em cache recebem um tempo de expiração, após o qual são invalidados e removidos do cache. Isso ajuda a garantir que dados obsoletos não sejam usados indefinidamente.\n",
        "\n",
        "#### **4.6. Políticas de Evicção de Cache:** Estratégias para determinar quais dados remover do cache quando ele atinge seu limite de armazenamento. Políticas comuns incluem:\n",
        "\n",
        "* **Last-In, First-Out (LIFO):** Os dados adicionados mais recentemente são os primeiros a serem removidos quando o cache precisa liberar espaço. Essa estratégia assume que os dados mais antigos provavelmente serão necessários novamente em breve.\n",
        "* **Least Recently Used (LRU):** Os dados acessados mais recentemente são os primeiros a serem removidos. Essa estratégia funciona bem quando os dados acessados mais recentemente têm maior probabilidade de serem acessados novamente.\n",
        "* **Most Recently Used (MRU):** Os dados acessados mais recentemente são os primeiros a serem removidos. Isso pode ser útil em cenários onde os dados mais recentes provavelmente serão usados apenas uma vez e não serão necessários novamente.\n",
        "* **Least Frequently Used (LFU):** Os dados que foram acessados o menor número de vezes são os primeiros a serem removidos. Essa estratégia ajuda a manter os dados mais frequentemente acessados no cache por mais tempo."
      ],
      "metadata": {
        "id": "gxK1jvk2v3-v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##5. Comparação de uso de Caching usando Fibonacci"
      ],
      "metadata": {
        "id": "3xnFNpv16WaG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###5.1. Sem Cache"
      ],
      "metadata": {
        "id": "pKAl05N5XefI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Este código define uma função recursiva para calcular o n-ésimo número de Fibonacci sem utilizar caching. A função chama a si mesma repetidamente, o que resulta em muitas chamadas redundantes e, portanto, em um desempenho lento para valores maiores de *n*."
      ],
      "metadata": {
        "id": "j_3iaqXR6sJp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Função para calcular o n-ésimo número de Fibonacci sem caching\n",
        "def fibonacci_no_cache(n):\n",
        "    if n <= 1:\n",
        "        return n\n",
        "    return fibonacci_no_cache(n-1) + fibonacci_no_cache(n-2)\n",
        "\n",
        "# Exemplo de uso da função\n",
        "print(f\"Fibonacci sem cache para n=10: {fibonacci_no_cache(10)}\")"
      ],
      "metadata": {
        "id": "7Z6R_nWz6Uun",
        "outputId": "fcc3789c-3df4-4d94-9184-5e4918a6e544",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fibonacci sem cache para n=10: 55\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###5.2. Com Cache"
      ],
      "metadata": {
        "id": "9ofpGgWjXhSV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Aqui, usamos o decorator *@cache* da biblioteca functools para armazenar os resultados de chamadas anteriores da função. Isso evita cálculos redundantes, já que o resultado para um determinado valor de n é armazenado na cache e reutilizado."
      ],
      "metadata": {
        "id": "zjLi2je17VbT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from functools import cache\n",
        "\n",
        "# Função Fibonacci com o decorator @cache\n",
        "@cache\n",
        "def fibonacci_cache(n):\n",
        "    if n <= 1:\n",
        "        return n\n",
        "    return fibonacci_cache(n-1) + fibonacci_cache(n-2)\n",
        "\n",
        "# Exemplo de uso da função\n",
        "print(f\"Fibonacci com cache para n=10: {fibonacci_cache(10)}\")"
      ],
      "metadata": {
        "id": "zz7qvjac51K5",
        "outputId": "2e537cca-36d7-46ec-8841-0b9d2fa4e448",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fibonacci com cache para n=10: 55\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###5.3. Com LRU Cache"
      ],
      "metadata": {
        "id": "Vol72rdyXkxP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "O decorator *@lru_cache* é semelhante ao *@cache*, mas permite definir um tamanho máximo para o cache. Quando o cache atinge este tamanho, os resultados menos recentemente usados são descartados para liberar espaço."
      ],
      "metadata": {
        "id": "xNbquCyi7V5g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from functools import lru_cache\n",
        "\n",
        "# Função Fibonacci com o decorator @lru_cache\n",
        "@lru_cache(maxsize=7)  # Limita o cache para os 7 resultados mais recentes\n",
        "def fibonacci_lru_cache(n):\n",
        "    if n <= 1:\n",
        "        return n\n",
        "    return fibonacci_lru_cache(n-1) + fibonacci_lru_cache(n-2)\n",
        "\n",
        "# Exemplo de uso da função\n",
        "print(f\"Fibonacci com LRU cache para n=10: {fibonacci_lru_cache(10)}\")"
      ],
      "metadata": {
        "id": "T_c6XbWi5-4w",
        "outputId": "d239c05b-cad4-436f-d40b-8c4b0233fecb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fibonacci com LRU cache para n=10: 55\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###5.4. Comparação de tempo de **chamadas** de função e análise dos resultados"
      ],
      "metadata": {
        "id": "vJu7BwFP5_zH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from functools import cache, lru_cache\n",
        "import timeit\n",
        "\n",
        "# Função sem cache\n",
        "def fibonacci_no_cache(n):\n",
        "    if n <= 1:\n",
        "        return n\n",
        "    return fibonacci_no_cache(n-1) + fibonacci_no_cache(n-2)\n",
        "\n",
        "# Função com cache\n",
        "@cache\n",
        "def fibonacci_cache(n):\n",
        "    if n <= 1:\n",
        "        return n\n",
        "    return fibonacci_cache(n-1) + fibonacci_cache(n-2)\n",
        "\n",
        "# Função com LRU cache\n",
        "@lru_cache\n",
        "def fibonacci_lru_cache(n):\n",
        "    if n <= 1:\n",
        "        return n\n",
        "    return fibonacci_lru_cache(n-1) + fibonacci_lru_cache(n-2)\n",
        "\n",
        "# Comparação de tempo de execução\n",
        "n = 35  # Define o valor de n para a comparação\n",
        "\n",
        "# Mede o tempo de execução sem cache\n",
        "no_cache_time = timeit.timeit(lambda: fibonacci_no_cache(n), number=1)\n",
        "\n",
        "# Mede o tempo de execução com cache\n",
        "cache_time = timeit.timeit(lambda: fibonacci_cache(n), number=1)\n",
        "\n",
        "# Mede o tempo de execução com LRU cache\n",
        "lru_cache_time = timeit.timeit(lambda: fibonacci_lru_cache(n), number=1)\n",
        "\n",
        "print(f\"Tempo sem cache: {no_cache_time:.6f} segundos\")\n",
        "print(f\"Tempo com cache: {cache_time:.6f} segundos\")\n",
        "print(f\"Tempo com LRU cache: {lru_cache_time:.6f} segundos\")"
      ],
      "metadata": {
        "id": "NbJq4fjh6EE2",
        "outputId": "020520b8-ac4f-41c6-ff07-2b20411ec0da",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tempo sem cache: 9.122187 segundos\n",
            "Tempo com cache: 0.000022 segundos\n",
            "Tempo com LRU cache: 0.000018 segundos\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Análise dos Resultados\n",
        "\n",
        "A análise dos tempos de execução das funções Fibonacci com e sem caching demonstra claramente os benefícios do uso de caching para melhorar o desempenho das aplicações.\n",
        "\n",
        "1. **Sem Caching:** A função sem caching (`fibonacci_no_cache`) leva significativamente mais tempo para executar, especialmente para valores maiores de `n`. Isso ocorre devido ao grande número de chamadas recursivas redundantes que precisam recalcular os mesmos valores repetidamente.\n",
        "\n",
        "2. **Com Caching (`@cache`):** O uso do decorator `@cache` armazena os resultados de chamadas anteriores em uma memória cache, evitando cálculos redundantes. Como resultado, a função com caching executa muito mais rápido, mostrando uma grande redução no tempo de execução.\n",
        "\n",
        "3. **Com LRU Caching (`@lru_cache`):** A função com o decorator `@lru_cache` não só oferece caching, mas também permite controlar o tamanho máximo do cache com o parâmetro `maxsize`. Isso é útil para gerenciar a memória e evitar o uso excessivo, mantendo os resultados mais recentemente utilizados. A execução é comparável à do caching simples, com a vantagem adicional de gerenciamento de cache."
      ],
      "metadata": {
        "id": "gtJK4O7X7Weu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##6. Utilização de Caching para Aumentar a Velocidade de uma Aplicação de LLM\n",
        "\n",
        "Além de melhorar o desempenho de funções recursivas como a função Fibonacci, o caching pode ser utilizado para acelerar significativamente aplicações que utilizam Modelos de Linguagem de Grande Escala (LLM). Em cenários de alta demanda, as chamadas para APIs de LLM, como o OpenAI GPT, podem ser lentas e custosas. O caching pode mitigar esses problemas ao armazenar respostas para consultas frequentes, reduzindo o tempo de resposta e o custo operacional.\n",
        "\n",
        "## Implementação de Caching com GPTCache\n",
        "\n",
        "Uma abordagem eficiente para caching de respostas de LLM é a utilização do pacote `GPTCache`, que constrói um cache semântico para armazenar respostas de LLM com base na similaridade dos inputs.\n",
        "\n",
        "### Exemplo de Implementação:\n",
        "\n",
        "Abaixo está um exemplo básico de como configurar e utilizar o `GPTCache` com uma API de LLM:\n"
      ],
      "metadata": {
        "id": "8rQJXKpHDOr-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Instalando dependencias\n",
        "!pip install openai\n",
        "!pip install gptcache"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nbfx1fQaEAR9",
        "outputId": "c3050458-46e3-45e5-95d5-6dd51281c4ce"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: openai in /usr/local/lib/python3.10/dist-packages (0.28.1)\n",
            "Requirement already satisfied: requests>=2.20 in /usr/local/lib/python3.10/dist-packages (from openai) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.5)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from openai) (3.10.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (3.8)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (2024.8.30)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (2.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (4.0.3)\n",
            "Requirement already satisfied: gptcache in /usr/local/lib/python3.10/dist-packages (0.1.44)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from gptcache) (1.26.4)\n",
            "Requirement already satisfied: cachetools in /usr/local/lib/python3.10/dist-packages (from gptcache) (5.5.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from gptcache) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->gptcache) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->gptcache) (3.8)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->gptcache) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->gptcache) (2024.8.30)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "from gptcache import cache\n",
        "from gptcache.adapter import openai\n",
        "import getpass\n",
        "import openai as openai_sdk\n",
        "\n",
        "# Prompt the user to enter the OpenAI API key securely\n",
        "api_key = getpass.getpass(prompt='Enter your OpenAI API key: ')\n",
        "\n",
        "# Initialize the GPTCache\n",
        "cache.init()\n",
        "\n",
        "# Set the OpenAI API key directly using the OpenAI SDK\n",
        "openai_sdk.api_key = api_key\n",
        "\n",
        "# Function to extract the response content\n",
        "def response_text(openai_resp):\n",
        "    return openai_resp['choices'][0]['message']['content']\n",
        "\n",
        "print(\"Cache loading...\")\n",
        "\n",
        "# Define the question to ask\n",
        "question = \"what's github\"\n",
        "\n",
        "# Loop to demonstrate caching\n",
        "for _ in range(2):\n",
        "    start_time = time.time()\n",
        "    response = openai.ChatCompletion.create(\n",
        "        model='gpt-3.5-turbo',\n",
        "        messages=[\n",
        "            {\n",
        "                'role': 'user',\n",
        "                'content': question\n",
        "            }\n",
        "        ],\n",
        "    )\n",
        "    print(f'Question: {question}')\n",
        "    print(\"Time consumed: {:.5f}s\".format(time.time() - start_time))\n",
        "    print(f'Answer: {response_text(response)}\\n')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xZSYGh_T5d7Y",
        "outputId": "5dfc2ccf-2993-47bc-d64c-7a273e36f385"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter your OpenAI API key: ··········\n",
            "Cache loading...\n",
            "Question: what's github\n",
            "Time consumed: 1.45835s\n",
            "Answer: GitHub is a web-based platform for version control using Git. It allows developers to collaborate on projects, track changes to code, and manage their code repositories. It is commonly used for open-source projects, but can also be used for private repositories. GitHub includes features such as code reviews, issue tracking, and project management tools.\n",
            "\n",
            "Question: what's github\n",
            "Time consumed: 0.00035s\n",
            "Answer: GitHub is a web-based platform for version control using Git. It allows developers to collaborate on projects, track changes to code, and manage their code repositories. It is commonly used for open-source projects, but can also be used for private repositories. GitHub includes features such as code reviews, issue tracking, and project management tools.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "T0b6-Z3foQwB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##7. Comparação de Caching em Consultas HTTP\n",
        "Neste exemplo, mostramos como o caching pode otimizar consultas HTTP repetidas. Comparamos três abordagens: sem cache, com cache manual implementado através de um decorador personalizado, e usando lru_cache para caching automático. O código mede o tempo de execução de cada abordagem para diferentes quantidades de requisições, evidenciando os benefícios do caching em reduzir o tempo de resposta e a carga sobre o servidor."
      ],
      "metadata": {
        "id": "-J5Q5afqerrW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importar os módulos necessários\n",
        "import time\n",
        "from functools import lru_cache, wraps\n",
        "import requests\n",
        "\n",
        "# Função para obter o conteúdo HTML\n",
        "def get_html_data(url):\n",
        "    response = requests.get(url)\n",
        "    return response.text\n",
        "\n",
        "# Implementação de Cache Manual usando um decorador\n",
        "def cache_manual(func):\n",
        "    cache = {}\n",
        "\n",
        "    @wraps(func)\n",
        "    def wrapper(*args):\n",
        "        if args in cache:\n",
        "            return cache[args]\n",
        "        else:\n",
        "            result = func(*args)\n",
        "            cache[args] = result\n",
        "            return result\n",
        "\n",
        "    wrapper.cache = cache  # Expondo o cache para poder limpar depois\n",
        "    return wrapper\n",
        "\n",
        "@cache_manual\n",
        "def get_html_data_cached(url):\n",
        "    response = requests.get(url)\n",
        "    return response.text\n",
        "\n",
        "# Cache usando LRU Cache\n",
        "@lru_cache(maxsize=None)\n",
        "def get_html_data_lru(url):\n",
        "    response = requests.get(url)\n",
        "    return response.text\n",
        "\n",
        "# Funções para resetar o cache\n",
        "def reset_manual_cache():\n",
        "    get_html_data_cached.cache.clear()\n",
        "\n",
        "def reset_lru_cache():\n",
        "    get_html_data_lru.cache_clear()\n",
        "\n",
        "# Função para medir o tempo de execução\n",
        "def measure_time(func, url, requests_count):\n",
        "    start_time = time.time()\n",
        "    for _ in range(requests_count):\n",
        "        func(url)\n",
        "    return time.time() - start_time\n",
        "\n",
        "# URLs e contagens\n",
        "url = 'https://books.toscrape.com/'\n",
        "requests_counts = [1, 10, 100, 500]\n",
        "\n",
        "# Medição de tempo para as três funções com reset de cache\n",
        "for count in requests_counts:\n",
        "    # Medindo o tempo para a função sem cache\n",
        "    normal_time = measure_time(get_html_data, url, count)\n",
        "\n",
        "    # Resetando e medindo o tempo para a função com cache manual\n",
        "    reset_manual_cache()\n",
        "    manual_cache_time = measure_time(get_html_data_cached, url, count)\n",
        "\n",
        "    # Resetando e medindo o tempo para a função com LRU cache\n",
        "    reset_lru_cache()\n",
        "    lru_cache_time = measure_time(get_html_data_lru, url, count)\n",
        "\n",
        "    print(f'Requisições: {count}')\n",
        "    print(f'Tempo (sem cache): {normal_time:.9f} segundos')\n",
        "    print(f'Tempo (cache manual): {manual_cache_time:.9f} segundos')\n",
        "    print(f'Tempo (LRU cache): {lru_cache_time:.9f} segundos\\n')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vShW6qjjfPhz",
        "outputId": "c18a782f-fa0f-418f-f604-9e1b08687cc3"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requisições: 1\n",
            "Tempo (sem cache): 0.149507523 segundos\n",
            "Tempo (cache manual): 0.073524475 segundos\n",
            "Tempo (LRU cache): 0.026438475 segundos\n",
            "\n",
            "Requisições: 10\n",
            "Tempo (sem cache): 0.218742847 segundos\n",
            "Tempo (cache manual): 0.019894361 segundos\n",
            "Tempo (LRU cache): 0.011918306 segundos\n",
            "\n",
            "Requisições: 100\n",
            "Tempo (sem cache): 1.577921152 segundos\n",
            "Tempo (cache manual): 0.043052197 segundos\n",
            "Tempo (LRU cache): 0.026732683 segundos\n",
            "\n",
            "Requisições: 500\n",
            "Tempo (sem cache): 7.365094662 segundos\n",
            "Tempo (cache manual): 0.011998177 segundos\n",
            "Tempo (LRU cache): 0.011190891 segundos\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##8. Conclusões:\n",
        "O uso de caching é uma estratégia eficaz para otimizar o desempenho de sistemas que dependem de consultas repetitivas, especialmente em aplicações que utilizam LLMs (Modelos de Linguagem de Grande Escala). Ao armazenar respostas de consultas repetidas ou semelhantes, o caching oferece múltiplos benefícios:\n",
        "\n",
        "* Eficiência no Tratamento de Repetições: O caching elimina a necessidade de processamento redundante, armazenando respostas para consultas repetitivas e melhorando a eficiência do sistema.\n",
        "* Redução de Latência: Reutilizando respostas previamente armazenadas, o caching reduz significativamente o tempo de resposta, o que é crucial para aplicações em tempo real.\n",
        "* Economia de Custos: Com menos chamadas diretas à API do LLM, o caching reduz os custos operacionais, tornando a abordagem mais econômica em aplicações de larga escala.\n",
        "* Aprimoramento da Experiência do Usuário: Respostas mais rápidas resultam em uma experiência de usuário superior, beneficiando especialmente aplicações como chatbots, assistentes virtuais e sistemas interativos.\n",
        "\n",
        "O uso de caching com GPTCache é especialmente útil em aplicações que requerem respostas frequentes e consistentes de LLMs, como chatbots, assistentes virtuais, e sistemas de recomendação. Esta abordagem não apenas melhora o desempenho, mas também permite uma melhor escalabilidade do sistema."
      ],
      "metadata": {
        "id": "4QQZyQL7De6W"
      }
    }
  ]
}